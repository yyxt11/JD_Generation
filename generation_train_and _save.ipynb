{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import helper\n",
    "import numpy as np\n",
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "int_text, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    建立NN网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.0.0\n",
      "Default GPU Device: /gpu:0\n"
     ]
    }
   ],
   "source": [
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Please use TensorFlow version 1.0 or newer'\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_inputs():\n",
    "    '''\n",
    "    输入初始化\n",
    "    '''\n",
    "    input_data = tf.placeholder(tf.int32,[None,None],name='input')\n",
    "    target_data = tf.placeholder(tf.int32,[None,None],name='target')\n",
    "    learning_rate = tf.placeholder(tf.float32,name='learning_rate')\n",
    "    return input_data, target_data, learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_init_cell(batch_size, rnn_size):\n",
    "    \"\"\"\n",
    "    初始化 RNN Cell.\n",
    "    :param batch_size: Size of batches\n",
    "    :param rnn_size: Size of RNNs\n",
    "    :return: Tuple (cell, initialize state)\n",
    "    \"\"\"\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([lstm]*2)\n",
    "    \n",
    "    initial_state = tf.identity(cell.zero_state(batch_size, tf.float32),name='initial_state')\n",
    "    \n",
    "    return cell, initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_embed(input_data, vocab_size, embed_dim):\n",
    "    \"\"\"\n",
    "    word embedding 输入.\n",
    "    :param input_data:  输入.\n",
    "    :param vocab_size: 总词语数.\n",
    "    :param embed_dim: w2v 维数\n",
    "    :return: Embedded input.\n",
    "    \"\"\"\n",
    "    #embedding 初始化，这边不采用预先训练的embeding,边训练边调参数\n",
    "    embedding = tf.Variable(tf.random_uniform((vocab_size,embed_dim),-1,1))\n",
    "    embed = tf.nn.embedding_lookup(embedding,input_data)\n",
    "    return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_rnn(cell, inputs):\n",
    "    \"\"\"\n",
    "    :param cell: RNN Cell\n",
    "    :param inputs: Input text data\n",
    "    :return: Tuple (Outputs, Final State)\n",
    "    \"\"\"\n",
    "    Outputs,Finalstate = tf.nn.dynamic_rnn(cell,inputs,dtype=tf.float32)\n",
    "    Final_state = tf.identity(Finalstate,\"final_state\")\n",
    "    return Outputs,Final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_nn(cell, rnn_size, input_data, vocab_size, embed_dim):\n",
    "    \"\"\"\n",
    "    :param cell: RNN cell\n",
    "    :param rnn_size: Size of rnns\n",
    "    :param input_data: Input data\n",
    "    :param vocab_size: Vocabulary size\n",
    "    :param embed_dim: Number of embedding dimensions\n",
    "    :return: Tuple (Logits, FinalState)\n",
    "    \"\"\"\n",
    "    embedding = get_embed(input_data,vocab_size,embed_dim)\n",
    "    lstm_output,final_state = build_rnn(cell,embedding)\n",
    "    #seq_output = tf.concat(lstm_output, axis=1)\n",
    "    #x = tf.reshape(seq_output,[-1,rnn_size])\n",
    "    #print(embedding.get_shape())\n",
    "    #print(lstm_output.get_shape())\n",
    "    \n",
    "    #weights = tf.Variable(tf.truncated_normal([lstm_output.get_shape()[0].value,lstm_output.get_shape()[2].value,vocab_size], stddev=0.1))\n",
    "    #bias = tf.Variable(tf.zeros(vocab_size))\n",
    "    \n",
    "    #print(weights.get_shape())\n",
    "    #logits = tf.matmul(lstm_output,weights)+ bias\n",
    "    logits = tf.contrib.layers.fully_connected(lstm_output,vocab_size,activation_fn=None)\n",
    "    return logits,final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(int_text, batch_size, seq_length):\n",
    "    \"\"\"\n",
    "    Return batches of input and target\n",
    "    :param int_text: Text with the words replaced by their ids\n",
    "    :param batch_size: The size of batch\n",
    "    :param seq_length: The length of sequence\n",
    "    :return: Batches as a Numpy array\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    batch_output = []\n",
    "    characters_per_batch = seq_length*batch_size\n",
    "    \n",
    "    #print(characters_per_batch)\n",
    "    batch_num = len(int_text)//characters_per_batch\n",
    "    \n",
    "    x_full_seqs = np.array(int_text[:batch_num*characters_per_batch])\n",
    "    \n",
    "    y_full_seqs = np.zeros_like(x_full_seqs)\n",
    "      \n",
    "    #bound limit\n",
    "    if len(int_text) > batch_num*characters_per_batch: \n",
    "        y_full_seqs = int_text[1:batch_num*characters_per_batch + 1]\n",
    "    else:\n",
    "        y_full_seqs[:-1],y_full_seqs[-1] = int_text[1:batch_num*characters_per_batch],int_text[0]\n",
    "    \n",
    "    #reshape\n",
    "    x_reshape = np.reshape(x_full_seqs,(batch_size,-1))\n",
    "    y_reshape = np.reshape(y_full_seqs,(batch_size,-1))\n",
    "    \n",
    "   # print(x_reshape)\n",
    "   # print(batch_num)\n",
    "    #individual batches\n",
    "    x_bathes = np.split(x_reshape,batch_num,1)\n",
    "    y_bathes = np.split(y_reshape,batch_num,1)\n",
    "                           \n",
    "   # print(x_bathes[0])\n",
    "   # print(y_bathes[0])\n",
    "   \n",
    "    for i in range(batch_num):  \n",
    "        batch_output.append(np.stack((x_bathes[i],y_bathes[i])))\n",
    "        \n",
    "    return np.array(batch_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#设置各种超参数\n",
    "num_epochs = 30\n",
    "batch_size = 512\n",
    "rnn_size = 512\n",
    "embed_dim = 256\n",
    "seq_length = 20\n",
    "learning_rate = 0.01\n",
    "\n",
    "#打印间隔\n",
    "show_every_n_batches = 10\n",
    "\n",
    "#保存路径\n",
    "save_dir = './save'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#build the graph\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    vocab_size = len(int_to_vocab)\n",
    "    input_text, targets, lr = get_inputs()\n",
    "    input_data_shape = tf.shape(input_text)\n",
    "    cell, initial_state = get_init_cell(input_data_shape[0], rnn_size)\n",
    "    logits, final_state = build_nn(cell, rnn_size, input_text, vocab_size, embed_dim)\n",
    "\n",
    "    # Probabilities for generating words\n",
    "    probs = tf.nn.softmax(logits, name='probs')\n",
    "\n",
    "    # Loss function\n",
    "    cost = seq2seq.sequence_loss(\n",
    "        logits,\n",
    "        targets,\n",
    "        tf.ones([input_data_shape[0], input_data_shape[1]]))\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "    # Gradient Clipping\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch    0/129   train_loss = 10.272\n",
      "Epoch   0 Batch   10/129   train_loss = 6.796\n",
      "Epoch   0 Batch   20/129   train_loss = 6.692\n",
      "Epoch   0 Batch   30/129   train_loss = 6.465\n",
      "Epoch   0 Batch   40/129   train_loss = 6.494\n",
      "Epoch   0 Batch   50/129   train_loss = 6.357\n",
      "Epoch   0 Batch   60/129   train_loss = 6.302\n",
      "Epoch   0 Batch   70/129   train_loss = 6.223\n",
      "Epoch   0 Batch   80/129   train_loss = 6.202\n",
      "Epoch   0 Batch   90/129   train_loss = 6.072\n",
      "Epoch   0 Batch  100/129   train_loss = 5.992\n",
      "Epoch   0 Batch  110/129   train_loss = 5.951\n",
      "Epoch   0 Batch  120/129   train_loss = 5.982\n",
      "Epoch   1 Batch    1/129   train_loss = 5.842\n",
      "Epoch   1 Batch   11/129   train_loss = 5.686\n",
      "Epoch   1 Batch   21/129   train_loss = 5.701\n",
      "Epoch   1 Batch   31/129   train_loss = 5.551\n",
      "Epoch   1 Batch   41/129   train_loss = 5.492\n",
      "Epoch   1 Batch   51/129   train_loss = 5.420\n",
      "Epoch   1 Batch   61/129   train_loss = 5.437\n",
      "Epoch   1 Batch   71/129   train_loss = 5.286\n",
      "Epoch   1 Batch   81/129   train_loss = 5.235\n",
      "Epoch   1 Batch   91/129   train_loss = 5.161\n",
      "Epoch   1 Batch  101/129   train_loss = 5.081\n",
      "Epoch   1 Batch  111/129   train_loss = 4.996\n",
      "Epoch   1 Batch  121/129   train_loss = 4.952\n",
      "Epoch   2 Batch    2/129   train_loss = 4.923\n",
      "Epoch   2 Batch   12/129   train_loss = 4.827\n",
      "Epoch   2 Batch   22/129   train_loss = 4.759\n",
      "Epoch   2 Batch   32/129   train_loss = 4.711\n",
      "Epoch   2 Batch   42/129   train_loss = 4.569\n",
      "Epoch   2 Batch   52/129   train_loss = 4.564\n",
      "Epoch   2 Batch   62/129   train_loss = 4.418\n",
      "Epoch   2 Batch   72/129   train_loss = 4.370\n",
      "Epoch   2 Batch   82/129   train_loss = 4.288\n",
      "Epoch   2 Batch   92/129   train_loss = 4.149\n",
      "Epoch   2 Batch  102/129   train_loss = 4.112\n",
      "Epoch   2 Batch  112/129   train_loss = 4.115\n",
      "Epoch   2 Batch  122/129   train_loss = 4.115\n",
      "Epoch   3 Batch    3/129   train_loss = 4.047\n",
      "Epoch   3 Batch   13/129   train_loss = 4.013\n",
      "Epoch   3 Batch   23/129   train_loss = 4.062\n",
      "Epoch   3 Batch   33/129   train_loss = 4.005\n",
      "Epoch   3 Batch   43/129   train_loss = 3.916\n",
      "Epoch   3 Batch   53/129   train_loss = 4.010\n",
      "Epoch   3 Batch   63/129   train_loss = 3.855\n",
      "Epoch   3 Batch   73/129   train_loss = 3.811\n",
      "Epoch   3 Batch   83/129   train_loss = 3.789\n",
      "Epoch   3 Batch   93/129   train_loss = 3.785\n",
      "Epoch   3 Batch  103/129   train_loss = 3.662\n",
      "Epoch   3 Batch  113/129   train_loss = 3.653\n",
      "Epoch   3 Batch  123/129   train_loss = 3.744\n",
      "Epoch   4 Batch    4/129   train_loss = 3.668\n",
      "Epoch   4 Batch   14/129   train_loss = 3.631\n",
      "Epoch   4 Batch   24/129   train_loss = 3.569\n",
      "Epoch   4 Batch   34/129   train_loss = 3.491\n",
      "Epoch   4 Batch   44/129   train_loss = 3.496\n",
      "Epoch   4 Batch   54/129   train_loss = 3.634\n",
      "Epoch   4 Batch   64/129   train_loss = 3.474\n",
      "Epoch   4 Batch   74/129   train_loss = 3.470\n",
      "Epoch   4 Batch   84/129   train_loss = 3.490\n",
      "Epoch   4 Batch   94/129   train_loss = 3.483\n",
      "Epoch   4 Batch  104/129   train_loss = 3.375\n",
      "Epoch   4 Batch  114/129   train_loss = 3.364\n",
      "Epoch   4 Batch  124/129   train_loss = 3.420\n",
      "Epoch   5 Batch    5/129   train_loss = 3.408\n",
      "Epoch   5 Batch   15/129   train_loss = 3.312\n",
      "Epoch   5 Batch   25/129   train_loss = 3.279\n",
      "Epoch   5 Batch   35/129   train_loss = 3.322\n",
      "Epoch   5 Batch   45/129   train_loss = 3.231\n",
      "Epoch   5 Batch   55/129   train_loss = 3.319\n",
      "Epoch   5 Batch   65/129   train_loss = 3.203\n",
      "Epoch   5 Batch   75/129   train_loss = 3.325\n",
      "Epoch   5 Batch   85/129   train_loss = 3.296\n",
      "Epoch   5 Batch   95/129   train_loss = 3.239\n",
      "Epoch   5 Batch  105/129   train_loss = 3.159\n",
      "Epoch   5 Batch  115/129   train_loss = 3.226\n",
      "Epoch   5 Batch  125/129   train_loss = 3.201\n",
      "Epoch   6 Batch    6/129   train_loss = 3.243\n",
      "Epoch   6 Batch   16/129   train_loss = 3.161\n",
      "Epoch   6 Batch   26/129   train_loss = 3.134\n",
      "Epoch   6 Batch   36/129   train_loss = 3.070\n",
      "Epoch   6 Batch   46/129   train_loss = 3.148\n",
      "Epoch   6 Batch   56/129   train_loss = 3.104\n",
      "Epoch   6 Batch   66/129   train_loss = 3.059\n",
      "Epoch   6 Batch   76/129   train_loss = 3.081\n",
      "Epoch   6 Batch   86/129   train_loss = 3.114\n",
      "Epoch   6 Batch   96/129   train_loss = 3.164\n",
      "Epoch   6 Batch  106/129   train_loss = 2.973\n",
      "Epoch   6 Batch  116/129   train_loss = 3.008\n",
      "Epoch   6 Batch  126/129   train_loss = 3.105\n",
      "Epoch   7 Batch    7/129   train_loss = 3.109\n",
      "Epoch   7 Batch   17/129   train_loss = 3.000\n",
      "Epoch   7 Batch   27/129   train_loss = 3.033\n",
      "Epoch   7 Batch   37/129   train_loss = 2.978\n",
      "Epoch   7 Batch   47/129   train_loss = 3.025\n",
      "Epoch   7 Batch   57/129   train_loss = 2.975\n",
      "Epoch   7 Batch   67/129   train_loss = 2.959\n",
      "Epoch   7 Batch   77/129   train_loss = 2.971\n",
      "Epoch   7 Batch   87/129   train_loss = 2.969\n",
      "Epoch   7 Batch   97/129   train_loss = 2.974\n",
      "Epoch   7 Batch  107/129   train_loss = 2.896\n",
      "Epoch   7 Batch  117/129   train_loss = 2.958\n",
      "Epoch   7 Batch  127/129   train_loss = 2.940\n",
      "Epoch   8 Batch    8/129   train_loss = 3.027\n",
      "Epoch   8 Batch   18/129   train_loss = 2.885\n",
      "Epoch   8 Batch   28/129   train_loss = 2.970\n",
      "Epoch   8 Batch   38/129   train_loss = 2.883\n",
      "Epoch   8 Batch   48/129   train_loss = 2.911\n",
      "Epoch   8 Batch   58/129   train_loss = 2.880\n",
      "Epoch   8 Batch   68/129   train_loss = 2.932\n",
      "Epoch   8 Batch   78/129   train_loss = 2.834\n",
      "Epoch   8 Batch   88/129   train_loss = 2.897\n",
      "Epoch   8 Batch   98/129   train_loss = 2.823\n",
      "Epoch   8 Batch  108/129   train_loss = 2.839\n",
      "Epoch   8 Batch  118/129   train_loss = 2.917\n",
      "Epoch   8 Batch  128/129   train_loss = 2.889\n",
      "Epoch   9 Batch    9/129   train_loss = 2.822\n",
      "Epoch   9 Batch   19/129   train_loss = 2.837\n",
      "Epoch   9 Batch   29/129   train_loss = 2.827\n",
      "Epoch   9 Batch   39/129   train_loss = 2.839\n",
      "Epoch   9 Batch   49/129   train_loss = 2.782\n",
      "Epoch   9 Batch   59/129   train_loss = 2.753\n",
      "Epoch   9 Batch   69/129   train_loss = 2.840\n",
      "Epoch   9 Batch   79/129   train_loss = 2.791\n",
      "Epoch   9 Batch   89/129   train_loss = 2.856\n",
      "Epoch   9 Batch   99/129   train_loss = 2.814\n",
      "Epoch   9 Batch  109/129   train_loss = 2.841\n",
      "Epoch   9 Batch  119/129   train_loss = 2.754\n",
      "Epoch  10 Batch    0/129   train_loss = 2.726\n",
      "Epoch  10 Batch   10/129   train_loss = 2.713\n",
      "Epoch  10 Batch   20/129   train_loss = 2.810\n",
      "Epoch  10 Batch   30/129   train_loss = 2.717\n",
      "Epoch  10 Batch   40/129   train_loss = 2.744\n",
      "Epoch  10 Batch   50/129   train_loss = 2.752\n",
      "Epoch  10 Batch   60/129   train_loss = 2.702\n",
      "Epoch  10 Batch   70/129   train_loss = 2.747\n",
      "Epoch  10 Batch   80/129   train_loss = 2.722\n",
      "Epoch  10 Batch   90/129   train_loss = 2.763\n",
      "Epoch  10 Batch  100/129   train_loss = 2.692\n",
      "Epoch  10 Batch  110/129   train_loss = 2.750\n",
      "Epoch  10 Batch  120/129   train_loss = 2.726\n",
      "Epoch  11 Batch    1/129   train_loss = 2.707\n",
      "Epoch  11 Batch   11/129   train_loss = 2.688\n",
      "Epoch  11 Batch   21/129   train_loss = 2.656\n",
      "Epoch  11 Batch   31/129   train_loss = 2.695\n",
      "Epoch  11 Batch   41/129   train_loss = 2.673\n",
      "Epoch  11 Batch   51/129   train_loss = 2.678\n",
      "Epoch  11 Batch   61/129   train_loss = 2.673\n",
      "Epoch  11 Batch   71/129   train_loss = 2.661\n",
      "Epoch  11 Batch   81/129   train_loss = 2.642\n",
      "Epoch  11 Batch   91/129   train_loss = 2.656\n",
      "Epoch  11 Batch  101/129   train_loss = 2.614\n",
      "Epoch  11 Batch  111/129   train_loss = 2.623\n",
      "Epoch  11 Batch  121/129   train_loss = 2.650\n",
      "Epoch  12 Batch    2/129   train_loss = 2.653\n",
      "Epoch  12 Batch   12/129   train_loss = 2.584\n",
      "Epoch  12 Batch   22/129   train_loss = 2.645\n",
      "Epoch  12 Batch   32/129   train_loss = 2.660\n",
      "Epoch  12 Batch   42/129   train_loss = 2.576\n",
      "Epoch  12 Batch   52/129   train_loss = 2.595\n",
      "Epoch  12 Batch   62/129   train_loss = 2.568\n",
      "Epoch  12 Batch   72/129   train_loss = 2.605\n",
      "Epoch  12 Batch   82/129   train_loss = 2.633\n",
      "Epoch  12 Batch   92/129   train_loss = 2.574\n",
      "Epoch  12 Batch  102/129   train_loss = 2.487\n",
      "Epoch  12 Batch  112/129   train_loss = 2.530\n",
      "Epoch  12 Batch  122/129   train_loss = 2.617\n",
      "Epoch  13 Batch    3/129   train_loss = 2.588\n",
      "Epoch  13 Batch   13/129   train_loss = 2.565\n",
      "Epoch  13 Batch   23/129   train_loss = 2.572\n",
      "Epoch  13 Batch   33/129   train_loss = 2.565\n",
      "Epoch  13 Batch   43/129   train_loss = 2.508\n",
      "Epoch  13 Batch   53/129   train_loss = 2.537\n",
      "Epoch  13 Batch   63/129   train_loss = 2.491\n",
      "Epoch  13 Batch   73/129   train_loss = 2.509\n",
      "Epoch  13 Batch   83/129   train_loss = 2.521\n",
      "Epoch  13 Batch   93/129   train_loss = 2.516\n",
      "Epoch  13 Batch  103/129   train_loss = 2.485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  13 Batch  113/129   train_loss = 2.507\n",
      "Epoch  13 Batch  123/129   train_loss = 2.524\n",
      "Epoch  14 Batch    4/129   train_loss = 2.500\n",
      "Epoch  14 Batch   14/129   train_loss = 2.508\n",
      "Epoch  14 Batch   24/129   train_loss = 2.494\n",
      "Epoch  14 Batch   34/129   train_loss = 2.454\n",
      "Epoch  14 Batch   44/129   train_loss = 2.431\n",
      "Epoch  14 Batch   54/129   train_loss = 2.537\n",
      "Epoch  14 Batch   64/129   train_loss = 2.435\n",
      "Epoch  14 Batch   74/129   train_loss = 2.457\n",
      "Epoch  14 Batch   84/129   train_loss = 2.471\n",
      "Epoch  14 Batch   94/129   train_loss = 2.499\n",
      "Epoch  14 Batch  104/129   train_loss = 2.398\n",
      "Epoch  14 Batch  114/129   train_loss = 2.441\n",
      "Epoch  14 Batch  124/129   train_loss = 2.420\n",
      "Epoch  15 Batch    5/129   train_loss = 2.478\n",
      "Epoch  15 Batch   15/129   train_loss = 2.455\n",
      "Epoch  15 Batch   25/129   train_loss = 2.425\n",
      "Epoch  15 Batch   35/129   train_loss = 2.448\n",
      "Epoch  15 Batch   45/129   train_loss = 2.370\n",
      "Epoch  15 Batch   55/129   train_loss = 2.473\n",
      "Epoch  15 Batch   65/129   train_loss = 2.405\n",
      "Epoch  15 Batch   75/129   train_loss = 2.469\n",
      "Epoch  15 Batch   85/129   train_loss = 2.400\n",
      "Epoch  15 Batch   95/129   train_loss = 2.399\n",
      "Epoch  15 Batch  105/129   train_loss = 2.333\n",
      "Epoch  15 Batch  115/129   train_loss = 2.379\n",
      "Epoch  15 Batch  125/129   train_loss = 2.378\n",
      "Epoch  16 Batch    6/129   train_loss = 2.432\n",
      "Epoch  16 Batch   16/129   train_loss = 2.364\n",
      "Epoch  16 Batch   26/129   train_loss = 2.383\n",
      "Epoch  16 Batch   36/129   train_loss = 2.318\n",
      "Epoch  16 Batch   46/129   train_loss = 2.404\n",
      "Epoch  16 Batch   56/129   train_loss = 2.395\n",
      "Epoch  16 Batch   66/129   train_loss = 2.364\n",
      "Epoch  16 Batch   76/129   train_loss = 2.382\n",
      "Epoch  16 Batch   86/129   train_loss = 2.381\n",
      "Epoch  16 Batch   96/129   train_loss = 2.449\n",
      "Epoch  16 Batch  106/129   train_loss = 2.321\n",
      "Epoch  16 Batch  116/129   train_loss = 2.355\n",
      "Epoch  16 Batch  126/129   train_loss = 2.405\n",
      "Epoch  17 Batch    7/129   train_loss = 2.384\n",
      "Epoch  17 Batch   17/129   train_loss = 2.311\n",
      "Epoch  17 Batch   27/129   train_loss = 2.370\n",
      "Epoch  17 Batch   37/129   train_loss = 2.326\n",
      "Epoch  17 Batch   47/129   train_loss = 2.349\n",
      "Epoch  17 Batch   57/129   train_loss = 2.356\n",
      "Epoch  17 Batch   67/129   train_loss = 2.336\n",
      "Epoch  17 Batch   77/129   train_loss = 2.344\n",
      "Epoch  17 Batch   87/129   train_loss = 2.361\n",
      "Epoch  17 Batch   97/129   train_loss = 2.368\n",
      "Epoch  17 Batch  107/129   train_loss = 2.338\n",
      "Epoch  17 Batch  117/129   train_loss = 2.334\n",
      "Epoch  17 Batch  127/129   train_loss = 2.316\n",
      "Epoch  18 Batch    8/129   train_loss = 2.399\n",
      "Epoch  18 Batch   18/129   train_loss = 2.269\n",
      "Epoch  18 Batch   28/129   train_loss = 2.404\n",
      "Epoch  18 Batch   38/129   train_loss = 2.274\n",
      "Epoch  18 Batch   48/129   train_loss = 2.326\n",
      "Epoch  18 Batch   58/129   train_loss = 2.319\n",
      "Epoch  18 Batch   68/129   train_loss = 2.391\n",
      "Epoch  18 Batch   78/129   train_loss = 2.283\n",
      "Epoch  18 Batch   88/129   train_loss = 2.353\n",
      "Epoch  18 Batch   98/129   train_loss = 2.289\n",
      "Epoch  18 Batch  108/129   train_loss = 2.327\n",
      "Epoch  18 Batch  118/129   train_loss = 2.333\n",
      "Epoch  18 Batch  128/129   train_loss = 2.316\n",
      "Epoch  19 Batch    9/129   train_loss = 2.285\n",
      "Epoch  19 Batch   19/129   train_loss = 2.282\n",
      "Epoch  19 Batch   29/129   train_loss = 2.338\n",
      "Epoch  19 Batch   39/129   train_loss = 2.301\n",
      "Epoch  19 Batch   49/129   train_loss = 2.234\n",
      "Epoch  19 Batch   59/129   train_loss = 2.235\n",
      "Epoch  19 Batch   69/129   train_loss = 2.354\n",
      "Epoch  19 Batch   79/129   train_loss = 2.277\n",
      "Epoch  19 Batch   89/129   train_loss = 2.326\n",
      "Epoch  19 Batch   99/129   train_loss = 2.320\n",
      "Epoch  19 Batch  109/129   train_loss = 2.337\n",
      "Epoch  19 Batch  119/129   train_loss = 2.277\n",
      "Epoch  20 Batch    0/129   train_loss = 2.205\n",
      "Epoch  20 Batch   10/129   train_loss = 2.212\n",
      "Epoch  20 Batch   20/129   train_loss = 2.287\n",
      "Epoch  20 Batch   30/129   train_loss = 2.245\n",
      "Epoch  20 Batch   40/129   train_loss = 2.250\n",
      "Epoch  20 Batch   50/129   train_loss = 2.281\n",
      "Epoch  20 Batch   60/129   train_loss = 2.262\n",
      "Epoch  20 Batch   70/129   train_loss = 2.288\n",
      "Epoch  20 Batch   80/129   train_loss = 2.248\n",
      "Epoch  20 Batch   90/129   train_loss = 2.336\n",
      "Epoch  20 Batch  100/129   train_loss = 2.234\n",
      "Epoch  20 Batch  110/129   train_loss = 2.310\n",
      "Epoch  20 Batch  120/129   train_loss = 2.270\n",
      "Epoch  21 Batch    1/129   train_loss = 2.249\n",
      "Epoch  21 Batch   11/129   train_loss = 2.208\n",
      "Epoch  21 Batch   21/129   train_loss = 2.172\n",
      "Epoch  21 Batch   31/129   train_loss = 2.244\n",
      "Epoch  21 Batch   41/129   train_loss = 2.230\n",
      "Epoch  21 Batch   51/129   train_loss = 2.229\n",
      "Epoch  21 Batch   61/129   train_loss = 2.226\n",
      "Epoch  21 Batch   71/129   train_loss = 2.228\n",
      "Epoch  21 Batch   81/129   train_loss = 2.216\n",
      "Epoch  21 Batch   91/129   train_loss = 2.233\n",
      "Epoch  21 Batch  101/129   train_loss = 2.199\n",
      "Epoch  21 Batch  111/129   train_loss = 2.209\n",
      "Epoch  21 Batch  121/129   train_loss = 2.247\n",
      "Epoch  22 Batch    2/129   train_loss = 2.226\n",
      "Epoch  22 Batch   12/129   train_loss = 2.157\n",
      "Epoch  22 Batch   22/129   train_loss = 2.211\n",
      "Epoch  22 Batch   32/129   train_loss = 2.229\n",
      "Epoch  22 Batch   42/129   train_loss = 2.156\n",
      "Epoch  22 Batch   52/129   train_loss = 2.175\n",
      "Epoch  22 Batch   62/129   train_loss = 2.173\n",
      "Epoch  22 Batch   72/129   train_loss = 2.181\n",
      "Epoch  22 Batch   82/129   train_loss = 2.224\n",
      "Epoch  22 Batch   92/129   train_loss = 2.184\n",
      "Epoch  22 Batch  102/129   train_loss = 2.131\n",
      "Epoch  22 Batch  112/129   train_loss = 2.144\n",
      "Epoch  22 Batch  122/129   train_loss = 2.185\n",
      "Epoch  23 Batch    3/129   train_loss = 2.234\n",
      "Epoch  23 Batch   13/129   train_loss = 2.162\n",
      "Epoch  23 Batch   23/129   train_loss = 2.176\n",
      "Epoch  23 Batch   33/129   train_loss = 2.178\n",
      "Epoch  23 Batch   43/129   train_loss = 2.122\n",
      "Epoch  23 Batch   53/129   train_loss = 2.104\n",
      "Epoch  23 Batch   63/129   train_loss = 2.110\n",
      "Epoch  23 Batch   73/129   train_loss = 2.169\n",
      "Epoch  23 Batch   83/129   train_loss = 2.157\n",
      "Epoch  23 Batch   93/129   train_loss = 2.163\n",
      "Epoch  23 Batch  103/129   train_loss = 2.143\n",
      "Epoch  23 Batch  113/129   train_loss = 2.161\n",
      "Epoch  23 Batch  123/129   train_loss = 2.152\n",
      "Epoch  24 Batch    4/129   train_loss = 2.118\n",
      "Epoch  24 Batch   14/129   train_loss = 2.135\n",
      "Epoch  24 Batch   24/129   train_loss = 2.120\n",
      "Epoch  24 Batch   34/129   train_loss = 2.111\n",
      "Epoch  24 Batch   44/129   train_loss = 2.079\n",
      "Epoch  24 Batch   54/129   train_loss = 2.144\n",
      "Epoch  24 Batch   64/129   train_loss = 2.060\n",
      "Epoch  24 Batch   74/129   train_loss = 2.106\n",
      "Epoch  24 Batch   84/129   train_loss = 2.143\n",
      "Epoch  24 Batch   94/129   train_loss = 2.188\n",
      "Epoch  24 Batch  104/129   train_loss = 2.089\n",
      "Epoch  24 Batch  114/129   train_loss = 2.121\n",
      "Epoch  24 Batch  124/129   train_loss = 2.069\n",
      "Epoch  25 Batch    5/129   train_loss = 2.131\n",
      "Epoch  25 Batch   15/129   train_loss = 2.120\n",
      "Epoch  25 Batch   25/129   train_loss = 2.075\n",
      "Epoch  25 Batch   35/129   train_loss = 2.106\n",
      "Epoch  25 Batch   45/129   train_loss = 2.082\n",
      "Epoch  25 Batch   55/129   train_loss = 2.131\n",
      "Epoch  25 Batch   65/129   train_loss = 2.094\n",
      "Epoch  25 Batch   75/129   train_loss = 2.128\n",
      "Epoch  25 Batch   85/129   train_loss = 2.094\n",
      "Epoch  25 Batch   95/129   train_loss = 2.109\n",
      "Epoch  25 Batch  105/129   train_loss = 2.028\n",
      "Epoch  25 Batch  115/129   train_loss = 2.055\n",
      "Epoch  25 Batch  125/129   train_loss = 2.083\n",
      "Epoch  26 Batch    6/129   train_loss = 2.113\n",
      "Epoch  26 Batch   16/129   train_loss = 2.054\n",
      "Epoch  26 Batch   26/129   train_loss = 2.035\n",
      "Epoch  26 Batch   36/129   train_loss = 2.013\n",
      "Epoch  26 Batch   46/129   train_loss = 2.087\n",
      "Epoch  26 Batch   56/129   train_loss = 2.069\n",
      "Epoch  26 Batch   66/129   train_loss = 2.061\n",
      "Epoch  26 Batch   76/129   train_loss = 2.082\n",
      "Epoch  26 Batch   86/129   train_loss = 2.097\n",
      "Epoch  26 Batch   96/129   train_loss = 2.139\n",
      "Epoch  26 Batch  106/129   train_loss = 2.033\n",
      "Epoch  26 Batch  116/129   train_loss = 2.082\n",
      "Epoch  26 Batch  126/129   train_loss = 2.104\n",
      "Epoch  27 Batch    7/129   train_loss = 2.064\n",
      "Epoch  27 Batch   17/129   train_loss = 2.031\n",
      "Epoch  27 Batch   27/129   train_loss = 2.084\n",
      "Epoch  27 Batch   37/129   train_loss = 2.065\n",
      "Epoch  27 Batch   47/129   train_loss = 2.077\n",
      "Epoch  27 Batch   57/129   train_loss = 2.066\n",
      "Epoch  27 Batch   67/129   train_loss = 2.076\n",
      "Epoch  27 Batch   77/129   train_loss = 2.041\n",
      "Epoch  27 Batch   87/129   train_loss = 2.101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  27 Batch   97/129   train_loss = 2.093\n",
      "Epoch  27 Batch  107/129   train_loss = 2.086\n",
      "Epoch  27 Batch  117/129   train_loss = 2.059\n",
      "Epoch  27 Batch  127/129   train_loss = 2.034\n",
      "Epoch  28 Batch    8/129   train_loss = 2.089\n",
      "Epoch  28 Batch   18/129   train_loss = 1.998\n",
      "Epoch  28 Batch   28/129   train_loss = 2.108\n",
      "Epoch  28 Batch   38/129   train_loss = 2.000\n",
      "Epoch  28 Batch   48/129   train_loss = 2.056\n",
      "Epoch  28 Batch   58/129   train_loss = 2.040\n",
      "Epoch  28 Batch   68/129   train_loss = 2.097\n",
      "Epoch  28 Batch   78/129   train_loss = 2.031\n",
      "Epoch  28 Batch   88/129   train_loss = 2.105\n",
      "Epoch  28 Batch   98/129   train_loss = 2.038\n",
      "Epoch  28 Batch  108/129   train_loss = 2.051\n",
      "Epoch  28 Batch  118/129   train_loss = 2.073\n",
      "Epoch  28 Batch  128/129   train_loss = 2.056\n",
      "Epoch  29 Batch    9/129   train_loss = 2.008\n",
      "Epoch  29 Batch   19/129   train_loss = 2.010\n",
      "Epoch  29 Batch   29/129   train_loss = 2.076\n",
      "Epoch  29 Batch   39/129   train_loss = 2.043\n",
      "Epoch  29 Batch   49/129   train_loss = 1.988\n",
      "Epoch  29 Batch   59/129   train_loss = 1.970\n",
      "Epoch  29 Batch   69/129   train_loss = 2.082\n",
      "Epoch  29 Batch   79/129   train_loss = 2.024\n",
      "Epoch  29 Batch   89/129   train_loss = 2.059\n",
      "Epoch  29 Batch   99/129   train_loss = 2.064\n",
      "Epoch  29 Batch  109/129   train_loss = 2.067\n",
      "Epoch  29 Batch  119/129   train_loss = 2.027\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "#训练\n",
    "batches = get_batches(int_text, batch_size, seq_length)\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(num_epochs):\n",
    "        state = sess.run(initial_state, {input_text: batches[0][0]})\n",
    "\n",
    "        for batch_i, (x, y) in enumerate(batches):\n",
    "            feed = {\n",
    "                input_text: x,\n",
    "                targets: y,\n",
    "                initial_state: state,\n",
    "                lr: learning_rate}\n",
    "            train_loss, state, _ = sess.run([cost, final_state, train_op], feed)\n",
    "\n",
    "            # Show every <show_every_n_batches> batches\n",
    "            if (epoch_i * len(batches) + batch_i) % show_every_n_batches == 0:\n",
    "                print('Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}'.format(\n",
    "                    epoch_i,\n",
    "                    batch_i,\n",
    "                    len(batches),\n",
    "                    train_loss))\n",
    "\n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_dir)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#参数保留\n",
    "helper.save_params((seq_length, save_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
